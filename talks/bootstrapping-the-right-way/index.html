<!DOCTYPE html><html lang="en"><head><title>Bootstrapping the right way</title><meta name="description" content="A brief overview of bootstrap sampling and ways to avoid common pitfalls when bootstrapping your data."><meta name="author" content="Yanir Seroussi"><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><link rel="stylesheet" href="css/reveal.css"><link rel="stylesheet" href="css/theme/night.css"><link rel="stylesheet" href="lib/css/zenburn.css"><link rel="stylesheet" href="custom.css"><!-- Print and PDF exports--><script>var link = document.createElement('link');
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName('head')[0].appendChild(link);
</script><!--[if lt IE 9]><script src="lib/js/html5shiv.js"></script><![endif]--></head><body><div class="reveal"><div class="slides"><section><h1>Bootstrapping the right way</h1><h4>Yanir Seroussi</h4><h6><a href="https://yanirseroussi.com" target="_blank">yanirseroussi.com</a>
|
<a href="https://twitter.com/yanirseroussi" target="_blank">@yanirseroussi</a>
|
<a href="https://linkedin.com/in/yanirseroussi" target="_blank">linkedin.com/in/yanirseroussi</a>
</h6></section><section><h2>Main takeaway</h2><img src="img/dont-believe-everything-you-read-on-the-internet-lincoln.jpg"><aside class="notes"><ul><li>Don't believe blog posts</li><li>Don't believe conference talks</li><li>Trust, but verify</li><li>When needed, go back to primary sources</li></ul></aside></section><section><h2>Running example: A/B testing</h2><div class="fragment"><p>We want to...</p><p class="indent no-margin neg-margin-top">...compare conversion rates and revenue</p><p class="indent no-margin">...estimate uncertainty of metrics</p></div><div class="fragment"><p>We don't want...</p><p class="indent no-margin neg-margin-top">...binary answers</p><p class="indent no-margin">...too many modelling assumptions</p></div><aside class="notes"><ul><li>Explain A/B testing where we care about both conversion rates and revenue</li><li>Uncertainty: We want a range for the effect size, not just a yes/no answer</li><li>Modelling assumptions: We need something that we can justify to ourselves and others</li><li>The rest of the talk basically follows this outline</li></ul></aside></section><section><section><h2>We want to compare...</h2><h1>conversion rates</h1></section><section><h2>Example: Testing titles for this talk</h2><table class="medium"><thead><tr><th>Variant</th><th>A</th><th>B</th></tr></thead><tbody><tr class="fragment"><td>Title</td><td>Rambling about bootstrapping, confidence intervals, and the reliability of online sources</td><td>Bootstrapping the right way</td></tr><tr class="fragment"><td>Visitors</td><td>2500</td><td>2500</td></tr><tr class="fragment"><td>Ticket sales</td><td>350</td><td>410</td></tr><tr class="fragment"><td>Conversion&nbsp;rate</td><td>14%</td><td>16.4%</td></tr></tbody></table></section><section><h2>Why are we doing this?</h2><img src="img/central-dogma-of-statistics.png"><div class="img-source-caption">&ndash; Joshua Akey,
<a href="https://www.gs.washington.edu/academics/courses/akey/56008/lecture/lecture1.pdf">Introduction to statistical genomics</a>
</div><aside class="notes"><ul><li>We have a population</li><li>We take a smaller sample from that population with probability</li><li>We use statistical inference to say something about the population</li><li>We care about the variability of our estimate for that population</li><li>We make some assumptions on experiments: random assignment, no selection bias, etc.</li></ul></aside></section><!--section--><!--  h2 Reminder #2: Experiments and other out-of-scope issues--><!----><!--  p Stuff from Hernan about when they do or do not work--><section><h2>Back to our problem: Ask the internet?</h2><blockquote><p>Let's look at a type of dataset that I often work on: conversions [...]
the formula for the confidence interval [...]</p><pre>n, k = 100, 3
scipy.stats.beta.ppf([0.025, 0.975], k, n-k)
</pre></blockquote><div class="img-source-caption">&ndash; Erik Bernhardsson,
<a href="https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html">The hacker's guide to uncertainty estimates</a>
</div></section><section><h2>Applying the formula to our case...</h2><img src="img/overlapping-confidence-intervals.png"><aside class="notes"><ul><li>Show of hands: What does this tell us?</li><li>Option 1: Difference isn't statistically significant &ndash; go with prior knowledge / easiest path / gut feeling</li><li>Option 2: This doesn't tell us much about the difference</li></ul></aside></section><section><h2>...but the difference CI can't be obtained from single-sample CIs!</h2><img src="img/overlapping-confidence-intervals-significant-difference.png"><aside class="notes"><ul><li>Emphasise that the confidence interval for the difference isn't simply a subtraction of the confidence intervals</li><li>Mention that we can also get the ratio or uplift if that's deemed more interesting, but the difference is more stable</li></ul></aside></section><!--section--><!--  h2 Side notes on conversions--><!----><!--  ul--><!--    li Closed formula solutions exist for difference and ratios???--><!--    li If you're happy to make assumptions, the Bayesian parametric calculator is the way to go--><!--    li Things get hairy if you care about more than the mean (e.g., comparing variances)--></section><section><section><h2>Detour:</h2><h1>On bootstrapping confidence intervals</h1></section><section><h2>What do CIs even mean?</h2><blockquote><p class="small">In statistics, a confidence interval (CI) is a type of interval estimate, computed from the statistics of
the observed data, that might contain the true value of an unknown population parameter. The interval has an
associated confidence level that, loosely speaking, quantifies the level of confidence that the parameter
lies in the interval. More strictly speaking, the confidence level represents the frequency (i.e. the
proportion) of possible confidence intervals that contain the true value of the unknown population
parameter. In other words, if confidence intervals are constructed using a given confidence level from an
infinite number of independent sample statistics, the proportion of those intervals that contain the true
value of the parameter will be equal to the confidence level.
</p></blockquote><div class="img-source-caption">&ndash; Wikipedia,
<a href="https://en.wikipedia.org/wiki/Confidence_interval">Confidence interval</a>
</div></section><section><img src="img/jackie-chan-confused.png"></section><section><h2>Let's try again...</h2><ul><li class="fragment">Interval: Not a single point, a range</li><li class="fragment">Confidence: Yet another confusing frequentist concept</li><li class="fragment">Confidence level: Higher is wider, lower is narrower</li><!--li.fragment Gelman: Call them uncertainty intervals--><li class="fragment">Key insight: We can test the correctness of CI algorithms!</li></ul><aside class="notes"><ul><li>Yet another: like statistical significance &ndash; misused and misunderstood</li><!--li.--><!--  <a href="https://statmodeling.stat.columbia.edu/2010/12/21/lets_say_uncert/">Gelman</a>: confidence--><!--  intervals are big in noisy situations where you have less confidence, and confidence intervals are small--><!--  when you have more confidence--><li>However, it gives us a general idea where the parameter is</li><li>Importantly, being a range, it doesn't hide uncertainty (unlike statistical significance)</li></ul></aside></section><section><h2>The basic bootstrap idea</h2><ol><li>Simulate the population by sampling with replacement <i>from your sample(s)</i></li><li>Calculate interesting stats based on the <i>re</i>samples</li></ol><img src="img/bootstrap-cartoon.jpg" height="300"><!--p You see this everywhere--><!--p Why should this work? A quick explanation of the theory--></section><section><h2>Bootstrapping by example: CI for mean</h2><table><thead><tr><td><b>Sample</b></td><td><b>Data</b></td><td><b>Mean</b></td></tr></thead><tbody><tr style="border-bottom: 1px solid #fff"><td>Original</td><td>10, 12, 20, 30, 45</td><td>23.4</td></tr><tr class="fragment"><td>Resample 1</td><td>30, 20, 12, 12, 45</td><td>23.8</td></tr><tr class="fragment"><td>Resample 2</td><td>20, 20, 30, 30, 30</td><td>26</td></tr><tr class="fragment"><td>...</td></tr></tbody></table><pre class="fragment">resample_means = [np.random.choice(sample, size=len(sample)).mean()
                  for _ in range(num_resamples)]
np.percentile(resample_means, [2.5, 97.5])
</pre></section><section><h2>What's wrong with the example?</h2><p>Small number of resamples (quote JVP, Erik -- too small!)</p><p>Show how the number of resamples matter</p><p>Percentile method -- show how it fails, small sample size</p><p>Quote Hesterberg</p></section><section><h2>Why accept anything as a given when we can simulate?</h2><p>The main reason for the popularity of bootstrapping is that it's easy to generate simulations</p><p>It's just as easy to test their validity</p><p>Simulation as the way to test that bootstrapping gives you the correct CI</p><p>Not all CIs are created equal (show Hesterberg results? or my own plots)</p></section><section><h2>A word of caution: Blog posts and talks < peer reviewed papers</h2><p>Pros of posts and talks: Easy to digest, straight to the point</p><p>Cons of posts and talks: Can mislead, usually not critiqued</p><p>Pros of papers: Peer review, higher bar for acceptance (for some)</p><p>Cons of papers: Can be hard to digest and mislead; peer review is only as good as the peers</p><p>Best approach: Cross-check multiple resources and test test test</p><p>Cite a few papers</p><p>Something about laziness -- myself included</p><p>Note the irony of me giving a talk on the topic?</p></section></section><section><section><h2>Back on course...</h2><h1>Comparing revenue</h1></section><section><h2>Data data data</h2><p>Simulation study: mixture of products with prices that vary geographically and people can purchase multiple products from each distribution</p><p>In general: can we learn about CI accuracy with real data? (e.g., population is purchases from 2018 &ndash; take different samples and see how well the CI works)</p></section><section><h2>Messier than conversions</h2><p>Bootstrap is better motivated</p><p>Show an example where we changed the price of a few products (or just one)</p></section><section><h2>Different methods, different CIs</h2><p>Use ARCH to get estimates (even sample size &ndash; 100, 1000, 10000)</p><p>Get true CI</p></section><section><h2>What about revenue over time?</h2><p>Bootstrap is IID! Add this if there's time</p></section></section><section><section><h2>Conclusion</h2><ul><li>Bootstrapping is a powerful tool, and it isn't as simple as it seems</li><li>Testing powerful tools is a good idea</li><li>Implementing from scratch is often a bad idea</li><li>Reliable resources can be hard to find, but they're out there (usually peer reviewed papers -- better go back to the originals)</li><li>Deeper understanding can come from Bayesian approaches</li><li>Further reading: https://yanirseroussi.com/2019/01/08/hackers-beware-bootstrap-sampling-may-be-harmful/</li></ul></section></section><!-- Abstract: Bootstrap sampling is being touted as a simple technique that any hacker can easily employ to--><!-- quantify the uncertainty of statistical estimates. However, despite its apparent simplicity, there are many--><!-- ways to misuse bootstrapping and thereby draw wrong conclusions about your data and the world. This talk gives--><!-- a brief overview of bootstrap sampling and discusses ways to avoid common pitfalls when bootstrapping your data.--><!----><!-- Proposal outline:--><!-- - An overview of bootstrap sampling, including how and why it works--><!-- - When one should and shouldn't use bootstrapping, including warnings about general issues like not drawing enough resamples--><!-- - A brief overview of confidence intervals, including the general definition, common misconceptions, and how to use them correctly--><!-- - Common bootstrapping methods to obtain confidence intervals--><!-- - How to make a decision whether the obtained confidence intervals are trustworthy--><!-- - Issues with bootstrapped confidence intervals and how to avoid them--><!-- TODO: 30 minutes including introductions and questions -- aim for 25 minutes?--><!-- TODO: stuff from https://yanirseroussi.com/2019/01/08/hackers-beware-bootstrap-sampling-may-be-harmful/--><section><h1>Questions?</h1></section></div></div><script src="lib/js/head.min.js"></script><script src="js/reveal.js"></script><script>Reveal.initialize({
  controls: true,
  controlsTutorial: false,
  progress: true,
  history: true,
  center: true,
  transition: 'slide',
  dependencies: [
    {
      src: 'lib/js/classList.js',
      condition: function() { return !document.body.classList; }
    },
    {
      src: 'plugin/markdown/marked.js',
      condition: function() { return !!document.querySelector( '[data-markdown]' ); }
    },
    {
      src: 'plugin/markdown/markdown.js',
      condition: function() { return !!document.querySelector( '[data-markdown]' ); }
    },
    {
      src: 'plugin/highlight/highlight.js',
      async: true,
      callback: function() { hljs.initHighlightingOnLoad(); }
    },
    {
      src: 'plugin/zoom-js/zoom.js',
      async: true
    },
    {
      src: 'plugin/notes/notes.js',
      async: true
    }
  ]
});</script></body></html>