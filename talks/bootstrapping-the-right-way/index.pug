doctype html
html(lang="en")
  head
    title Bootstrapping the right way

    meta(name="description",
         content="A brief overview of bootstrap sampling and ways to avoid common pitfalls when bootstrapping your data.")
    meta(name="author", content="Yanir Seroussi")
    meta(charset="utf-8")
    meta(name="viewport", content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no")
    meta(name="apple-mobile-web-app-capable", content="yes")
    meta(name="apple-mobile-web-app-status-bar-style", content="black-translucent")

    link(rel="stylesheet", href="css/reveal.css")
    link(rel="stylesheet", href="css/theme/night.css")
    link(rel="stylesheet", href="lib/css/zenburn.css")
    link(rel="stylesheet", href="custom.css")

    // Print and PDF exports
    script.
      var link = document.createElement('link');
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName('head')[0].appendChild(link);

    <!--[if lt IE 9]><script src="lib/js/html5shiv.js"></script><![endif]-->

  body
    .reveal: .slides
      section
        h1 Bootstrapping the right way
        h4 Yanir Seroussi
        h6.
          <a href="https://yanirseroussi.com" target="_blank">yanirseroussi.com</a>
          |
          <a href="https://twitter.com/yanirseroussi" target="_blank">@yanirseroussi</a>
          |
          <a href="https://linkedin.com/in/yanirseroussi" target="_blank">linkedin.com/in/yanirseroussi</a>

      section
        h2 Main takeaway

        img(src="img/dont-believe-everything-you-read-on-the-internet-lincoln.jpg")

        aside.notes: ul
          li Don't believe blog posts
          li Don't believe conference talks
          li Trust, but verify
          li When needed, go back to primary sources

      section
        h2 Running example: A/B testing

        .fragment
          p We want to...
          p.indent.no-margin.neg-margin-top ...compare conversion rates and revenue
          p.indent.no-margin ...estimate uncertainty of metrics

        .fragment
          p We don't want...
          p.indent.no-margin.neg-margin-top ...binary answers
          p.indent.no-margin ...too many modelling assumptions

        aside.notes: ul
          li Explain A/B testing where we care about both conversion rates and revenue
          li Uncertainty: We want a range for the effect size, not just a yes/no answer
          li Modelling assumptions: We need something that we can justify to ourselves and others
          li The rest of the talk basically follows this outline

      section
        section
          h2 We want to compare...
          h1 conversion rates

        section
          h2 Example: Testing titles for this talk

          table.medium
            thead
              tr
                th Variant
                th A
                th B
            tbody
              tr.fragment
                td Title
                td Rambling about bootstrapping, confidence intervals, and the reliability of online sources
                td Bootstrapping the right way
              tr.fragment
                td Visitors
                td 2500
                td 2500
              tr.fragment
                td Ticket sales
                td 350
                td 410
              tr.fragment
                td Conversion&nbsp;rate
                td 14%
                td 16.4%

        section
          h2 Why are we doing this?

          img(src="img/central-dogma-of-statistics.png")

          .img-source-caption.
            &ndash; Joshua Akey,
            <a href="https://www.gs.washington.edu/academics/courses/akey/56008/lecture/lecture1.pdf">Introduction to statistical genomics</a>

          aside.notes: ul
            li We have a population
            li We take a smaller sample from that population with probability
            li We use statistical inference to say something about the population
            li We care about the variability of our estimate for that population
            li We make some assumptions on experiments: random assignment, no selection bias, etc.

        //section
        //  h2 Reminder #2: Experiments and other out-of-scope issues
        //
        //  p Stuff from Hernan about when they do or do not work

        section
          h2 Back to our problem: Ask the internet?

          blockquote
            p.
              Let's look at a type of dataset that I often work on: conversions [...]
              the formula for the confidence interval [...]
            pre.
              n, k = 100, 3
              scipy.stats.beta.ppf([0.025, 0.975], k, n-k)

          .img-source-caption.
            &ndash; Erik Bernhardsson,
            <a href="https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html">The hacker's guide to uncertainty estimates</a>

        section
          h2 Applying the formula to our case...

          img(src="img/overlapping-confidence-intervals.png")

          aside.notes: ul
            li Show of hands: What does this tell us?
            li Option 1: Difference isn't statistically significant &ndash; go with prior knowledge / easiest path / gut feeling
            li Option 2: This doesn't tell us much about the difference

        section
          h2 ...but the difference CI can't be obtained from single-sample CIs!

          img(src="img/overlapping-confidence-intervals-significant-difference.png")

          aside.notes: ul
            li Emphasise that the confidence interval for the difference isn't simply a subtraction of the confidence intervals
            li Mention that we can also get the ratio or uplift if that's deemed more interesting, but the difference is more stable

        //section
        //  h2 Side notes on conversions
        //
        //  ul
        //    li Closed formula solutions exist for difference and ratios???
        //    li If you're happy to make assumptions, the Bayesian parametric calculator is the way to go
        //    li Things get hairy if you care about more than the mean (e.g., comparing variances)

      section
        section
          h2 Detour:
          h1 On bootstrapping confidence intervals

        section
          h2 What do CIs even mean?

          blockquote: p.small.
            In statistics, a confidence interval (CI) is a type of interval estimate, computed from the statistics of
            the observed data, that might contain the true value of an unknown population parameter. The interval has an
            associated confidence level that, loosely speaking, quantifies the level of confidence that the parameter
            lies in the interval. More strictly speaking, the confidence level represents the frequency (i.e. the
            proportion) of possible confidence intervals that contain the true value of the unknown population
            parameter. In other words, if confidence intervals are constructed using a given confidence level from an
            infinite number of independent sample statistics, the proportion of those intervals that contain the true
            value of the parameter will be equal to the confidence level.

          .img-source-caption.
            &ndash; Wikipedia,
            <a href="https://en.wikipedia.org/wiki/Confidence_interval">Confidence interval</a>

        section
          img(src="img/jackie-chan-confused.png")

        section
          h2 Let's try again...

          ul
            li.fragment Interval: Not a single point, a range
            li.fragment Confidence: Yet another confusing frequentist concept
            li.fragment Confidence level: Higher is wider, lower is narrower
            //li.fragment Gelman: Call them uncertainty intervals
            li.fragment Key insight: We can test the correctness of CI algorithms!

          aside.notes: ul
            li Yet another: like statistical significance &ndash; misused and misunderstood
            //li.
            //  <a href="https://statmodeling.stat.columbia.edu/2010/12/21/lets_say_uncert/">Gelman</a>: confidence
            //  intervals are big in noisy situations where you have less confidence, and confidence intervals are small
            //  when you have more confidence
            li However, it gives us a general idea where the parameter is
            li Importantly, being a range, it doesn't hide uncertainty (unlike statistical significance)

        section
          h2 The basic bootstrap idea

          ol
            li Simulate the population by sampling with replacement <i>from your sample(s)</i>
            li Calculate interesting stats based on the <i>re</i>samples

          img(src="img/bootstrap-cartoon.jpg", height=300)

          //p You see this everywhere
          //p Why should this work? A quick explanation of the theory

        section
          h2 Bootstrapping by example: CI for mean

          table
            thead
              tr
                td: b Sample
                td: b Data
                td: b Mean
            tbody
              tr(style="border-bottom: 1px solid #fff")
                td Original
                td 10, 12, 20, 30, 45
                td 23.4
              tr.fragment
                td Resample 1
                td 30, 20, 12, 12, 45
                td 23.8
              tr.fragment
                td Resample 2
                td 20, 20, 30, 30, 30
                td 26
              tr.fragment
                td ...

          pre.fragment.
            resample_means = [np.random.choice(sample, size=len(sample)).mean()
                              for _ in range(num_resamples)]
            np.percentile(resample_means, [2.5, 97.5])

        section
          h2 What's wrong with the example?

          p Small number of resamples (quote JVP, Erik -- too small!)
          p Show how the number of resamples matter
          p Percentile method -- show how it fails, small sample size
          p Quote Hesterberg

        section
          h2 Why accept anything as a given when we can simulate?

          p The main reason for the popularity of bootstrapping is that it's easy to generate simulations
          p It's just as easy to test their validity
          p Simulation as the way to test that bootstrapping gives you the correct CI
          p Not all CIs are created equal (show Hesterberg results? or my own plots)

        section
          h2 A word of caution: Blog posts and talks < peer reviewed papers

          p Pros of posts and talks: Easy to digest, straight to the point
          p Cons of posts and talks: Can mislead, usually not critiqued
          p Pros of papers: Peer review, higher bar for acceptance (for some)
          p Cons of papers: Can be hard to digest and mislead; peer review is only as good as the peers
          p Best approach: Cross-check multiple resources and test test test
          p Cite a few papers
          p Something about laziness -- myself included
          p Note the irony of me giving a talk on the topic?

      section
        section
          h2 Back on course...
          h1 Comparing revenue

        section
          h2 Data data data

          p Simulation study: mixture of products with prices that vary geographically and people can purchase multiple products from each distribution
          p In general: can we learn about CI accuracy with real data? (e.g., population is purchases from 2018 &ndash; take different samples and see how well the CI works)

        section
          h2 Messier than conversions

          p Bootstrap is better motivated
          p Show an example where we changed the price of a few products (or just one)

        section
          h2 Different methods, different CIs

          p Use ARCH to get estimates (even sample size &ndash; 100, 1000, 10000)
          p Get true CI

        section
          h2 What about revenue over time?

          p Bootstrap is IID! Add this if there's time

      section
        section
          h2 Conclusion

          ul
            li Bootstrapping is a powerful tool, and it isn't as simple as it seems
            li Testing powerful tools is a good idea
            li Implementing from scratch is often a bad idea
            li Reliable resources can be hard to find, but they're out there (usually peer reviewed papers -- better go back to the originals)
            li Deeper understanding can come from Bayesian approaches
            li Further reading: https://yanirseroussi.com/2019/01/08/hackers-beware-bootstrap-sampling-may-be-harmful/

      // Abstract: Bootstrap sampling is being touted as a simple technique that any hacker can easily employ to
      // quantify the uncertainty of statistical estimates. However, despite its apparent simplicity, there are many
      // ways to misuse bootstrapping and thereby draw wrong conclusions about your data and the world. This talk gives
      // a brief overview of bootstrap sampling and discusses ways to avoid common pitfalls when bootstrapping your data.
      //
      // Proposal outline:
      // - An overview of bootstrap sampling, including how and why it works
      // - When one should and shouldn't use bootstrapping, including warnings about general issues like not drawing enough resamples
      // - A brief overview of confidence intervals, including the general definition, common misconceptions, and how to use them correctly
      // - Common bootstrapping methods to obtain confidence intervals
      // - How to make a decision whether the obtained confidence intervals are trustworthy
      // - Issues with bootstrapped confidence intervals and how to avoid them

      // TODO: 30 minutes including introductions and questions -- aim for 25 minutes?
      // TODO: stuff from https://yanirseroussi.com/2019/01/08/hackers-beware-bootstrap-sampling-may-be-harmful/

      section
        h1 Questions?

    script(src="lib/js/head.min.js")
    script(src="js/reveal.js")
    script.
      Reveal.initialize({
        controls: true,
        controlsTutorial: false,
        progress: true,
        history: true,
        center: true,
        transition: 'slide',
        dependencies: [
          {
            src: 'lib/js/classList.js',
            condition: function() { return !document.body.classList; }
          },
          {
            src: 'plugin/markdown/marked.js',
            condition: function() { return !!document.querySelector( '[data-markdown]' ); }
          },
          {
            src: 'plugin/markdown/markdown.js',
            condition: function() { return !!document.querySelector( '[data-markdown]' ); }
          },
          {
            src: 'plugin/highlight/highlight.js',
            async: true,
            callback: function() { hljs.initHighlightingOnLoad(); }
          },
          {
            src: 'plugin/zoom-js/zoom.js',
            async: true
          },
          {
            src: 'plugin/notes/notes.js',
            async: true
          }
        ]
      });
